# 스프링 카프카 프로듀서 컨슈머




## 프로듀서

## application.yml

아래는 application.yml 에 설정할 수 있는 프로듀서 옵션 값이다. (application.yml 파일에 설정한 프로듀서에 대한 옵션 값은 애플리케이션이 실행될 때 자동으로 오버라이드 되어 설정된다) <br>

> 스프링 카프카에서 사용할 수 있는 속성들은 https://docs.spring.io/spring-boot/docs/current/reference/html/application-properties.html 에서 확인 가능하다.

```properties
spring.kafka.producer.acks
spring.kafka.producer.batch-size
spring.kafka.producer.bootstrap-servers
spring.kafka.producer.buffer-memory
spring.kafka.producer.client-id
spring.kafka.producer.compression-type
spring.kafka.producer.key-serializer
spring.kafka.producer.properties.*
spring.kafka.producer.retries
spring.kafka.producer.transaction-id-prefix
spring.kafka.producer.value-serializer
```

스프링 카프카가 아닌 일반 카프카 라이브러리를 자바의 기본 기능만을 이용해서 구현할 때는 반드시 `bootstrap-servers`, `key-serializer`, `value-serializer`를 선언하지 않으면 ConfigException 이 발생한다. 이 3개는 필수 옵션이기 때문이다.<br>

스프링 카프카에서 프로듀서를 사용할 경우는 필수 옵션이 없다. 만약  `bootstrap-servers`, `key-serializer`, `value-serializer` 에 값을 설정하지 않으면 localhost:9092, StringSerializer 로 각각 기본값이 자동 설정된다.<br>

아래는 acks 옵션을 all 로 설정하고 host를 직접 지정한 경우의 application.yml 파일이다.

```yaml
spring:
  kafka:
    producer:
      bootstrap-servers: ec2-gosgjung-hotmail:9092
      acks: all
```



## 주의 사항

- 토픽 이름에는 `-` 가 들어가면 안된다.



## 프로듀서

### 실습) 첫번째 프로듀서 애플리케이션

가장 기본으로 제공되는 것을 커스터마이징하지 않고 단순 동작만을 확인하기 위한 예제를 작성해보면 아래와 같다.

```java
package io.study.springkafka.config;

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.CommandLineRunner;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.stereotype.Component;

@Component
public class SampleKafkaProducerRunner implements CommandLineRunner {

	@Autowired
	private KafkaTemplate<Integer, String> template;

	@Override
	public void run(String... args) throws Exception {
		for(int i=0; i<10; i++){
			template.send("spring-kafka-test", String.format("테스트메시지#%s", i));
		}
		System.exit(0);
	}
}
```

위의 코드를 인텔리제이에서 Run Application 한 후에 카프카가 설치된 EC2 또는 로컬 개발 PC에서 메시지를 확인해보면 아래와 같다.<br>

아래는 로컬 개발 PC에서 카프카 컨슈머 CLI를 실행시킨 결과다.<br>

**컨슈머 CLI로 결과 확인해보기**<br>

```bash
$ bin/kafka-console-consumer.sh --bootstrap-server ec2-gosgjung-hotmail:9092 --topic spring-kafka-test --property print.key=true --property key.separator="-" --from-beginning


null-테스트메시지#0
null-테스트메시지#1
null-테스트메시지#2
null-테스트메시지#3
null-테스트메시지#4
null-테스트메시지#5
null-테스트메시지#6
null-테스트메시지#7
null-테스트메시지#8
null-테스트메시지#9
```

<br>

### 실습) 프로듀서 - kafkaTemplate.send() 메서드의 다양한 형태

스프링 카프카에서 제공해주는 KafkaTemplate 에서 제공해주는 send 메세지는 아래와 같은 종류가 있다.

- send(String topic, V data)
  - 토픽에 데이터 data 를 전송한다.
- send(String topic, Integer partition, K key, V data)
  - 메시지 키, 메시지 값을 포함하는 레코드를 특정 토픽의 특정 파티션으로 전달
- send(String topic, Integer partition, Long timestamp, K key, V data)
  - 메시지 키, 메시지 값, 타임스템프를 포함하는 레코드를 특정 토픽의 특정 파티션으로 전달
- send(ProducerRecord\<K,V\> record)
  - 프로듀서 레코드(ProducerRecord) 객체를 전송

<br>

## 커스텀 카프카 템플릿 정의

하나의 스프링 카프카 애플리케이션 내부에 다양한 종류의 카프카 프로듀서 인스턴스를 생성하고 싶을 경우에 사용하는 방식이다. 예를 들면 클러스터 A 로 데이터를 전송하는 카프카 프로듀서, 클러스터 B로 데이터를 전송하는 카프카 프로듀서를 애플리케이션에서 KafkaTemplate 객체로 사용하고 싶을 경우에 각각의 설정을 다르게 해서 각기 다른 빈으로 등록해 사용할 수 있다.<br>

### Configuration

```java
package io.study.springkafkastart.config;

import java.util.HashMap;
import java.util.Map;

import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.common.serialization.StringSerializer;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.core.DefaultKafkaProducerFactory;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.kafka.core.ProducerFactory;

@Configuration
public class CustomKafkaTemplateConfig {

	@Bean
	public KafkaTemplate<String, String> customKafkaTemplate(){
		Map<String, Object> prop = new HashMap<>();
		prop.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "ec2-gosgjung-hotmail:9092");
		prop.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
		prop.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
		prop.put(ProducerConfig.ACKS_CONFIG, "all");

		ProducerFactory<String, String> producerFactory = new DefaultKafkaProducerFactory<>(prop);
		return new KafkaTemplate<>(producerFactory);
	}
}

```

<br>

### 샘플 프로듀서 코드

```java
package io.study.springkafkastart.config.runner;

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.CommandLineRunner;
import org.springframework.kafka.core.KafkaProducerException;
import org.springframework.kafka.core.KafkaSendCallback;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.kafka.support.SendResult;
import org.springframework.stereotype.Component;
import org.springframework.util.concurrent.ListenableFuture;

import lombok.extern.slf4j.Slf4j;

@Slf4j
@Component
public class CustomKafkaTemplateRunner implements CommandLineRunner {

	private static String TOPIC_NAME = "KAFKA_SAMPLE";

	@Autowired
	private KafkaTemplate<String, String> customKafkaTemplate;

	@Override
	public void run(String... args) throws Exception {

		ListenableFuture<SendResult<String, String>> future = customKafkaTemplate.send(TOPIC_NAME, "커스텀 카프카 템플릿 메시지입니다~");

		future.addCallback(new KafkaSendCallback<String, String>(){
			@Override
			public void onFailure(KafkaProducerException ex) {
				log.info("Failure 발생 (onFailure) >>> " + ex.getMessage());
			}

			@Override
			public void onSuccess(SendResult<String, String> result) {
				log.info("메시지 전송 성공(onSuccess) >>> " + String.valueOf(result));
			}
		});

		System.exit(0);
	}
}
```

<br>


## 컨슈머

카프카 클라이언트 라이브러리로 컨슈머를 직접 구현할 때 가장 어려운 부분은 커밋을 구현하는 부분이다. 카프카 컨슈머에서 직접 커밋을 구현할 때는 오토 커밋, 동기 커밋, 비동기 커밋으로 크게 세가지로 나뉘어진다. 하지만, 실제 운영환경에서는 다양한 종류의 커밋을 구현해서 사용해야 한다. 예를 들면 특정 타이밍마다 커밋을 하거나 레코드 갯수에 따라 커밋을 하는 규칙을 적용할 때는 로직을 새로 작성해야 한다.<br>



**스프링 카프카의 컨슈머**<br>

스프링 카프카는 카프카 라이브러리를 스프링에서 사용하기 편리하도록 만들어진 라이브러리다. 기존 컨슈머를 2개의 타입으로 나누고 커밋을 7가지 유형으로 분류해놓았다.리스너의 종류에 따라 한번 호출하는 메서드에서 처리할 수 있는 레코드의 갯수가 달라진다.<br>



**스프링 카프카의 두가지 리스너**<br>

스프링 카프카의 기본설정으로 제공되는 리스너 타입은 레코드 리스너다. 기본적인 리스너이기에, 필요에 따라 수정해서 사용해야 한다. 필요할 경우 배치 메시지 리스너를 사용해야 할 경우가 굉장히 자주 있을 듯 하다.<br>

- 레코드 리스너(Message Listener)
  - 한번에 한개의 레코드를 단건으로 처리하는 리스너다.
- 배치 리스너(BatchMessageListener)
  - Spring AMQP 로 ActiveMQ, RabbitMQ를 다뤄봤다면 배치리스너가 생소하지는 않을 것 같다. 
  - 스프링 카프카 역시도 BatchMessageListener 를 제공해주고 있다.
  - poll() 메서드로 리턴받은 ConsumerRecords 와 같이 한버넹 여러개의 레코드들을 전달받아 처리할 수 있다.



**스프링 카프카에서 제공하는 편의성 메시지 리스너들**<br>

- **Acknowledging**MessageListener
- **ConsumerAware**MessageListener
- **AcknowledgingConsumerAware**MessageListener
- Batch**Acknowledging**MessageListener
- Batch**ConsumerAware**MessageListener
- Batch**AcknowledgingConsumerAware**MessageListener

<br>

- Acknowledging 이 붙은 리스너들 
  - 매뉴얼 커밋을 사용하려고 할때 주로 사용한다.
- ConsumerAware 가 붙은 리스너들
  - KafkaConsumer 인스턴스에 직접 접근해서 컨트롤하려고 할때 주로 사용한다.
- AcknowledgingConsumerAware 가 붙은 리스너들
  - Acknowledging 과 ConsumerAware 의 기능이 모두 포함한 기능을 제공한다.
  - 메뉴얼 커밋을 사용하면서, KafkaConsumer에도 직접 붙어서 컨트롤하고자 할 때 사용

<br>

**스프링 카프카에서의 커밋 개념은 AckMode**<br>

스프링 카프카는 사용자가 사용할 만한 커밋의 종류를 7가지로 세분화하고 미리 로직을 만들어놓았다. 사용자는 각 커밋 종류를 살펴보고 어떤 커밋 방식을 사용할지 정하면 된다. **스프링 카프카에서는 커밋이라는 용어를 `AckMode` 라고 부른다.** 이 점에 유의해야 한다.<br>



**프로듀서의 acks 와 컨슈머의 AckMode, 두 개념 혼동하지 말아야**<br>

프로듀서에서 사용하는 옵션 `acks` 과 컨슈머가 사용하는 옵션인 `AckMode` 는 둘 다 Acknowledgement 를 연상시키기에 혼동하기 쉽다. 프로듀서에서 사용하는 `acks` 와 컨슈머에서 사용하는 `AckMode` 는 명백히 다른 옵션이어서 `acks` 와 `AckMode` 를 혼동하지 않아야 한다. 스프링 카프카 컨슈머의 `AckMode` 의 기본 설정 값은 `BATCH` 다. 컨슈머의 `enable.auto.commit` 옵션의 기본 값은 false 로 지정된다.<br>

> 참고로 위의 프로듀서 예제에서 `acks` 옵션은 아래와 같이 지정했었다.
>
> - `spring.kafka.producer.acks` : all

<br>

스프링 카프카에서는 사용자가 사용할 만한 커밋의 종류를 7가지(RECORD, BATCH, TIME, COUNT, COUNT_TIME, MANUAL, MANUAL_IMMEDIATE)로 세분화해서 미리 로직을 만들어놓았다.

### AckMode 7 가지

| AckMode         | 용도 및 역할, 기본값                                         |
| --------------- | ------------------------------------------------------------ |
| RECORD          | 레코드 단위로 처리(프로세싱) 후 커밋                         |
| BATCH           | poll() 메서드로 호출된 레코드가 모두 처리된 이후에 커밋. 스프링 카프카 컨슈머의 AckMode 기본값 |
| TIME            | 특정 시간 이후에 커밋. 이 옵션을 사용할 경우, 시간 간격을 의미하는 AckTime 옵션을 지정해야 함 |
| COUNT           | 특정 갯수만큼 레코드가 처리된 이후에 커밋. 이 옵션을 사용할 경우, 레코드 갯수를 선언하는 AckCount 옵션을 지정해야 함 |
| COUNT_TIME      | TIME, COUNT 옵션 중 맞는 조건이 둘중에 하나라도 있을 경우 커밋을 수행 |
| MANUAL          | Acknowledgement.acknowledge() 메서드가 호출되면 다음번 poll() 에서 커밋을 한다. <br>매번 acknowledge() 메서드를 호출하면 BATCH 옵션과 동일하게 동작한다. <br>이 옵션을 사용할 경우 AcknowledgingMessageListener 또는 BatchAcknowledgingMessageListener 를 리스너로 사용해야 한다. |
| MANUAL_IMMEDATE | Acknwledgement.acknowledge() 메서드를 호출한 즉시 커밋한다. 이 옵션을 사용할 경우 AcknowledgingMessageListener 또는 BatchAcknowledgingMessageListener 를 리스너로 사용해야 한다. |

<br>

### RECORD, BATCH 리스너의 종류들

> 참고) MANUAL 커밋
>
> - MANUAL 커밋을 사용하는 것은 보통 Acknowledging 이 붙은 메시지리스너를 사용하는 경우에 MANUAL 커밋을 수행한다.
> - 다음번 poll() 을 수행할 때 커밋을 수행하게 된다.
> - Acknowledging 이 붙은 메시지리스너는 보통 Acknowledgement.acknowledge() 메서드를 사용하는데, Acknowledgement.acknoledge() 메서드가 호출되면 다음번 poll() 에서 커밋을 한다.
> - Acknowledgement.acknowledge() 메서드가 호출되면 다음번 poll() 에서 커밋을 한다. 



|        |                                                    |                                                              |
| ------ | -------------------------------------------------- | ------------------------------------------------------------ |
| RECORD | MessageListener                                    | Record 인스턴스 단위로 메시지를 리슨<br>오토커밋 또는 컨슈머 컨테이너의 AckMode를 사용하는 경우에 사용 |
| RECORD | **Acknowledging**MessageListener                   | Record 인스턴스 단위로 메시지를 리슨 <br>매뉴얼 커밋을 수행한다. <br>Acknowledging 이 붙어있는 클래스들은 주로 매뉴얼 커밋을 수행한다는 것을 기억하자. <br>즉, 레코드 단위로 메시지를 받는데, 매뉴얼 커밋을 수행한다. |
| RECORD | **ConsumerAware**MessageListener                   | Record 인스턴스 단위로 메시지를 리슨<br>Acknowledging 이 붙어있는 클래스들은 주로 매뉴얼 커밋을 수행한다는 것을 기억하자. <br>컨슈머 객체에 직접 접근해서 사용하고자 할 때 사용한다. |
| RECORD | **AcknowledgingConsumerAware**MessageListener      | Record 인스턴스 단위로 메시지를 리슨<br>Acknowledging 이 붙어있는 클래스들은 주로 매뉴얼 커밋을 수행한다는 것을 기억하자. <br>레코드 단위로 메시지를 받으면서, 컨슈머에 직접 접근 하면서, 매뉴얼 커밋 역시 수행한다. |
| BATCH  | BatchMessageListener                               | Record**s** 단위로 메시지를 리슨한다. <br>(Record 가 아니라 Records 다. 여러개의 메시지를 한묶음으로(Batch) 받는다) <br>오토 커밋 또는 컨슈머 컨테이너의 AckMode를 사용한다. |
| BATCH  | Batch**Acknowledging**MessageListener              | Record**s** 단위로 메시지를 리슨한다. <br/>(Record 가 아니라 Records 다. 여러개의 메시지를 한묶음으로(Batch) 받는다) <br/>Records 단위로 메시지를 받는데, 매뉴얼 커밋을 수행한다. |
| BATCH  | Batch**ConsumerAware**MessageListener              | Record**s** 단위로 메시지를 리슨한다. <br/>(Record 가 아니라 Records 다. 여러개의 메시지를 한묶음으로(Batch) 받는다) <br/>Records 단위로 메시지를 받는데, 컨슈머 컨테이너에 직접 접근해서 리슨하고자 할 때 사용한다. |
| BATCH  | Batch**AcknowledgingConsumerAware**MessageListener | Record**s** 단위로 메시지를 리슨한다. <br/>(Record 가 아니라 Records 다. 여러개의 메시지를 한묶음으로(Batch) 받는다) <br/>Records 단위로 메시지를 받는데, 컨슈머 컨테이너에 직접 접근해서, 매뉴얼 커밋을 사용한 리슨 방식을 사용한다. |

### application.yml

> 참고) http://bit.ly/3qOaxQd

```yaml
spring.kafka.consumer.auto-commit-interval
spring.kafka.consumer.auto-offset-reset
spring.kafka.consumer.bootstrap-servers
spring.kafka.consumer.client-id
spring.kafka.consumer.enable-auto-commit
spring.kafka.consumer.fetch-max-wait
spring.kafka.consumer.fetch-min-size
spring.kafka.consumer.group-id
spring.kafka.consumer.heartbeat-interval
spring.kafka.consumer.key-deserializer
spring.kafka.consumer.max-poll-records
spring.kafka.consumer.properties.*
spring.kafka.consumer.value-deserializer
spring.kafka.consumer.ack-count
spring.kafka.consumer.ack-mode
spring.kafka.consumer.ack-time
spring.kafka.consumer.client-id
spring.kafka.consumer.concurrency
spring.kafka.consumer.idle-event-interval
spring.kafka.consumer.log-container-config
spring.kafka.consumer.monitor-interval
spring.kafka.consumer.no-poll-threshold
spring.kafka.consumer.listener.poll-timeout
spring.kafka.consumer.listener.type
```

<br>

### 샘플 application.yml

> 스프링 카프카에서 사용할 수 있는 속성들은 https://docs.spring.io/spring-boot/docs/current/reference/html/application-properties.html 에서 확인 가능하다.

```yaml
spring:
  kafka:
    consumer:
      bootstrap-servers: ec2-gosgjung-hotmail:9092
    listener:
      ack-mode: manual_immediate
      # singe | batch
      type: single
```

위의 설정은 bootstrap-servers 를 ec2-gosgjung-hotmail:9092 로 설정하고, listener 의 ack-mode 를 manual_intermediate 로 설정했다. 그리고 spring.kafka.consumer.type 은 single 로 설정했다. spring.kafka.consumer.type 은 batch, single 두가지 방식으로 설정 가능한데, 이 옵션은 spring-boot 2.5.5 의 기본 카프카 버전에서 적용되는 버전이다. 이전 버전에서는 RECORD/BATCH 로 지정 가능하다.<br>

<br>

### 실습) 첫번째 컨슈머 애플리케이션 - 기본 리스너 컨테이너

<br>

#### 레코드 리스너 (싱글 메시지 리스너)

##### application.yml

```yaml
spring:
  kafka:
    consumer:
      bootstrap-servers: ec2-gosgjung-hotmail:9092
    listener:
      ack-mode: manual_immediate
      # singe | batch
      type: single
      # 또는 RECORD 로 지정 (2.5.5 이하 버전 중 특정 버전)
      # type: RECORD
```



##### 리스너 코드

```java
package io.study.springkafkastart.config.listener;

import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.kafka.annotation.PartitionOffset;
import org.springframework.kafka.annotation.TopicPartition;
import org.springframework.stereotype.Component;

import lombok.extern.slf4j.Slf4j;

@Slf4j
@Component
public class KafkaBasicSingleListener {

	public static final String TOPIC_NAME = "KAFKA_SAMPLE";

	@KafkaListener(
		topics = TOPIC_NAME,
		groupId = "test-group00"
	)
	public void recordListener00(ConsumerRecord<String, String> record){
		log.info(record.toString());
	}

	@KafkaListener(
		topics = TOPIC_NAME,
		groupId = "test-group01"
	)
	public void recordListener01(String messageValue){
		log.info(messageValue);
	}

	@KafkaListener(
		topics = TOPIC_NAME,
		groupId = "test-group02",
		properties = {
			"max.poll.interval.ms:60000",
			"auto.offset.reset:earliest"
		}
	)
	public void singleTopicWithPropertiesListener(String messageValue){
		log.info(messageValue);
	}

	@KafkaListener(
		topics = TOPIC_NAME,
		groupId = "test-group03",
		concurrency = "3"
	)
	public void concurrentTopicListener(String messageValue){
		log.info(messageValue);
	}

	@KafkaListener(
		topicPartitions = {
			@TopicPartition(topic = "test01", partitions = {"0", "1"}),
			@TopicPartition(topic = "test02", partitionOffsets = @PartitionOffset(partition = "0", initialOffset = "3"))
		},
		groupId = "test-group04"
	)
	public void listenSpecificPartition(ConsumerRecord<String, String> record){
		log.info(record.toString());
	}
}

```



- 가장 기본적인 리스너 선언이다. @KafkaListener 애노테이션 옵션내에 topics, groupId 를 지정했다. 토픽 ID, 그룹 ID를 지정가능하다. 
- poll() 이 호출되어 가져온 레코드 들은 차례대로 개별 레코드의 메시지 값을 파라미터로 받게 된다.
- `recordListener01(String messageValue)`
  - 메시지의 Value 를 그대로 파라미터로 받았다. 역직렬화 클래스를 StringDeserializer 로 지정해주었기에 String 타입의 메시지 값(Value)를 역직렬화 해 받아낼 수 있다.
- `singleTopicWithPropertiesListener(String messageValue)`
  - @KafkaListenr 내에 properties 필드에 대해 프로퍼티들을 명시적으로 지정해주었다.
  - 이렇게 개별 리스너 메서드에 속성을 정해주고 싶을때 특정 리스너에 직접 속성을 지정하는 것이 가능하다. (이 부분은 확실히 편리하고, 사용자를 고려한 속성인 것 같다는 느낌.)
- `concurrentTopicListener(String messageValue)`
  - @KafkaListener 내에 concurrency 필드에 대해 3 을 지정해주었다.
  - 두개 이상의 카프카 컨슈머 스레드를 실행하려고 한다면 concurrency 옵션을 사용하면 된다.
  - concurrency 에 지정한 숫자만큼, 컨슈머 스레드가 생성되어 병렬처리된다.
  - 예를 들면 파티션이 10개인 토픽을 구독할 때 가장 효율을 내기 위해 concurrency 를 10으로 지정할 수 있다. 이렇게 설정하면 10개 파티션에 컨슈머 스레드가 가각 할당되어 병렬 처리량이 늘어난다.
- `listenSpecificPartition(ConsumerRecord<String, String> record)`
  - 특정 토픽의 특정 파티션만 구독하려 한다면 topicPartitions 파라미터를 사용한다.
  - 여기에 추가로 PartitionOffset 어노테이션을 활용하면 특정 파티션의 특정 오프셋까지 지정할 수 있다.
  - 이 경우 그룹 아이디에 상관 없이 항상 설정한 오프셋의 데이터부터 가져온다.

<br>

#### 배치  컨슈머 리스너

> 참고) 배치 컨슈머 리스너(BatchConsumerAwareMessageListener)와 배치 커밋 리스너(BatchAknowledgingMessageListener)는 다르다. 용어가 비슷해서 혼동되수 있으니 유의하자.

##### application.yml

```yaml
spring:
  kafka:
    producer:
      bootstrap-servers: ec2-gosgjung-hotmail:9092
      acks: all
    consumer:
      bootstrap-servers: ec2-gosgjung-hotmail:9092
    listener:
      ack-mode: manual_immediate
      # singe | batch
      type: batch
```

일단 여기까지 정리하게 되었는데, 이번 프로젝트에서는 카프카를 사용하게 되지 않을 것 같다. 직접 t2.medium 급의 개인 서버에서 예제를 여러개 돌려봤었고 나도 카프카를 돌려보고 싶은 마음은 굴뚝 같은데, 데이터 전달 구조를 계속 그림을 그려보고 단순화하고 리팩토링을 자주 거치다 보니 캐시를 적극적으로 활용하면서 단순하게 구조를 만드는 더 좋은 구조를 생각해내게 되었다. 그래서 일단은 그 구조로 계속 해볼 예정이다.<br>

그리고 나중에 미국주식이 아닌, 한국주식, 베트남, 대만 까지 트래픽을 견디려면 어는 순간에는 카프카를 도입하게 될 것 같은데, 이때를 대비해서 계속 공부하게 될 것 같다. 카프카를 쓰는 이유는 단순하다. 순서를 보장하지 않아도 되는 작업 대기열이 필요하기도 하고,  페이지 캐시를 사용하기에 메모리 관점에서 장점이 있고, 자체적으로 파티셔닝이 되고, 레플케이션, 클러스터링이 가능하기에, 언젠가 더 큰 대역폭을 가지려 할때 한번 도입해볼만하다고 생각이든다.<br>

이번 실 테스트 기간에는 조금이나마 시간이 남는다면, 조금의 동작하는 기능구현을 남겨놓아야 겠다는 생각을 하고 있다. 조건변수만 변경하면 작동되게끔 기능을 구현해놓을 수 있도록 준비해두었는데, 3시간 정도만 뽕맞은 것처럼 미친척하고 일하면, 외부 데이터 연동은 단순 기능 동작은 검증된 상태로 마무리 해놓은 상태로 다음 개발을 준비하게 될수 잇을 것 같다.<br>

어제는 새벽 2시 반까지 일하다가 오전 6시 반에 깨서 오... 늦었다 ㅁㅊㄷ... 하고서 벌떡일어나서 출근했는데, 진짜 오늘 힘들었다. 8시 사무실 출근해서 7시까지 일하고 집으로 출발했다. 오늘은 조금 쉬어야 내일 또 빡시게 할수 있겠다는 생각이 들어서 오늘은 여기까지만 정리.<br>

<br>

아.. 맞다. 오늘 정합성 관련 테스트 코드를 완성했다. 테스트 구조도 이것 저것 직접 공수해서 만들었고, 테스트 코드를 만들다보니 테스트가 가능하도록 코드를 만드느라 리팩토링하느라 시간도 많이 걸리기도 했고, 테스트를 하다보니 에러들을 굉장히 많이 찾게 되어 시간은 조금 걸린 편이었지만, 한번 이렇게 테스트를 한벌 만들어놓으면 나중에 버튼한번만 눌러서 검증이 되기에 장점이 있는듯 하다. 예전 고참들이 테스트를 강조하기도 했고, 대표님 면접 때도 테스트 코드에 대한 개념에 대한 질문도 있을 정도로 전 직장에서는 테스트를 강조했었는데, 아직 그때 수준의 테스트 코드까지 도달하려면 한참 멀었다. 워낙에 모든 것들을 직접 공수해서 만들다보니, 다 갖춰져 있던 예전 회사의 코드 수준에는 아직 못미치는 것 같다. 

### 실습) 커스텀 리스너 컨테이너









